{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "092a5342",
   "metadata": {},
   "source": [
    "# Implementing RepVGG in PyTorch\n",
    "## Make your cnn > 100x faster!\n",
    "\n",
    "Hello There!! Today we’ll see how to implement RepVGG in PyTorch proposed in [RepVGG: Making VGG-style ConvNets Great Again](https://arxiv.org/pdf/2101.03697.pdf)\n",
    "\n",
    "Code is here, an interactive version of this article can be downloaded from here.\n",
    "\n",
    "Let’s get started!\n",
    "\n",
    "The paper proposed a new architecture that can be tuned after training in order to make it faster on modern hardware. And by faster I mean really fast, this idea was used by [Apple's MobileOne model].(https://arxiv.org/abs/2206.04040).\n",
    "\n",
    "![img](images/fig1.png)\n",
    "\n",
    "\n",
    "## Single vs Multi Branch Models\n",
    "\n",
    "A lot of recent models, use multi branching, were the input is passed trought different layers and then aggregated somehow (usually with addition).\n",
    "\n",
    "![img](images/single_multi_branch.png)\n",
    "\n",
    "\n",
    "\n",
    "This is great because it makes the multi branch model an implicit ensemble of numerous shallower models. More specifically, *the model can be interpreted as an ensemble of 2^n models, since every block branches the flow into two paths.*\n",
    "\n",
    "Unfortunately, multi branch models consume more memory and are slower compared to single branch. Let's create a classic `ResNetBlock` to see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad1faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torchvision.ops import Conv2dNormActivation\n",
    "from typing import Dict, List\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Sequential(\n",
    "            Conv2dNormActivation(\n",
    "                in_channels, out_channels, kernel_size=3, stride=stride\n",
    "            ),\n",
    "            Conv2dNormActivation(\n",
    "                out_channels, out_channels, kernel_size=3, activation_layer=None\n",
    "            ),\n",
    "        )\n",
    "        self.shortcut = (\n",
    "            Conv2dNormActivation(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=1,\n",
    "                stride=stride,\n",
    "                activation_layer=None,\n",
    "            )\n",
    "            if in_channels != out_channels\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.shortcut(x)  # <- 2x memory\n",
    "        x = self.weight(x)\n",
    "        x += res\n",
    "        x = self.act(x)  # <- 1x memory\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0278731f",
   "metadata": {},
   "source": [
    "When we store the `residual`, we are consuming `2x` memory. This is also shown in the following image from the paper\n",
    "\n",
    "![img](images/fig3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cf5487",
   "metadata": {},
   "source": [
    "The authors noticed that the **multi branch architecture is usefull only at train time**. Thus, if we can have a way to remove it at test time we can improve the model speed and memory comsumption.\n",
    "\n",
    "\n",
    "### From Multi Branches to Single Branch\n",
    "\n",
    "Consider the following situation, you have two branches composed of two `3x3` convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a359e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoBranches(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x)\n",
    "        return x1 + x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406a4e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 5, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_branches = TwoBranches(8, 8)\n",
    "\n",
    "x = torch.randn((1, 8, 7, 7))\n",
    "\n",
    "two_branches(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c249f0d",
   "metadata": {},
   "source": [
    "Now, we can actually create one conv, let's call it `conv_fused` such that `conv_fused(x) = conv1(x) + conv2(x)`. Very easily, we can just sum up the `weight`s and the `bias`s of the two convs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "915dcd24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv1 = two_branches.conv1\n",
    "conv2 = two_branches.conv2\n",
    "\n",
    "conv_fused = nn.Conv2d(conv1.in_channels, conv1.out_channels, kernel_size=conv1.kernel_size)\n",
    "\n",
    "conv_fused.weight = nn.Parameter(conv1.weight + conv2.weight)\n",
    "conv_fused.bias =  nn.Parameter(conv1.bias + conv2.bias)\n",
    "\n",
    "# check they give the same output\n",
    "assert torch.allclose(two_branches(x), conv_fused(x), atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d73b63c",
   "metadata": {},
   "source": [
    "Let's see how much faster it is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a68f16d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1(x) + conv2(x) tooks 0.000381s\n",
      "conv_fused(x) tooks 0.000207s\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "two_branches.to(\"cuda\")\n",
    "conv_fused.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.randn((4, 8, 7, 7), device=torch.device(\"cuda\"))\n",
    "    \n",
    "    start = perf_counter()\n",
    "    two_branches(x)\n",
    "    print(f\"conv1(x) + conv2(x) tooks {perf_counter() - start:.6f}s\")\n",
    "    \n",
    "    start = perf_counter()\n",
    "    conv_fused(x)\n",
    "    print(f\"conv_fused(x) tooks {perf_counter() - start:.6f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900fb1c6",
   "metadata": {},
   "source": [
    "Almost `50%` less (keep in mind this is a very naive benchmark, we will see a better one later on)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a4d921",
   "metadata": {},
   "source": [
    "### Fuse Conv and BatchNorm\n",
    "\n",
    "In practice, we use `BatchNorm` as a regularization layer. We can actually create a conv such that `conv_fused(x) = batchnorm(conv(x))`. The idea is to change the weights of `conv` in order to incorporation the shifting and scaling from `BatchNorm`. \n",
    "\n",
    "The paper explain it as follows:\n",
    "\n",
    "![img](images/eq3.png)\n",
    "\n",
    "The code is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e36bd071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fused_bn_to_conv_state_dict(\n",
    "    conv: nn.Conv2d, bn: nn.BatchNorm2d\n",
    ") -> Dict[str, Tensor]:\n",
    "    # in the paper, weights is gamma and bias is beta\n",
    "    bn_mean, bn_var, bn_gamma, bn_beta = (\n",
    "        bn.running_mean,\n",
    "        bn.running_var,\n",
    "        bn.weight,\n",
    "        bn.bias,\n",
    "    )\n",
    "    # we need the std!\n",
    "    bn_std = (bn_var + bn.eps).sqrt()\n",
    "    # eq (3)\n",
    "    conv_weight = nn.Parameter((bn_gamma / bn_std).reshape(-1, 1, 1, 1) * conv.weight)\n",
    "    # still eq (3)\n",
    "    conv_bias = nn.Parameter(bn_beta - bn_mean * bn_gamma / bn_std)\n",
    "    return {\"weight\": conv_weight, \"bias\": conv_bias}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264f3f5d",
   "metadata": {},
   "source": [
    "Let's see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d6ac0bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_bn = nn.Sequential(\n",
    "    nn.Conv2d(8, 8, kernel_size=3, bias=False),\n",
    "    nn.BatchNorm2d(8)\n",
    ")\n",
    "\n",
    "torch.nn.init.uniform_(conv_bn[1].weight)\n",
    "torch.nn.init.uniform_(conv_bn[1].bias)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # be sure to switch to eval mode!!\n",
    "    conv_bn = conv_bn.eval()\n",
    "    conv_fused = nn.Conv2d(conv_bn[0].in_channels, \n",
    "                           conv_bn[0].out_channels, \n",
    "                           kernel_size=conv_bn[0].kernel_size)\n",
    "\n",
    "    conv_fused.load_state_dict(get_fused_bn_to_conv_state_dict(conv_bn[0], conv_bn[1]))\n",
    "\n",
    "    x = torch.randn((1, 8, 7, 7))\n",
    "    \n",
    "    assert torch.allclose(conv_bn(x), conv_fused(x), atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d691a33d",
   "metadata": {},
   "source": [
    "yes, we fused a `Conv2d` and a `BatchNorm2d` layer together. There is also an [article from PyTorch about this](https://pytorch.org/tutorials/intermediate/custom_function_conv_bn_tutorial.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dd3346",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "So our goal is to fuse all the branches in one single conv, making the network faster! \n",
    "\n",
    "The author proposed a new type of block, called `RepVGG`. Similar to ResNet, it has a shortcut but it also has an identity connection (or bette branch).\n",
    "\n",
    "\n",
    "![img](images/fig2.png)\n",
    "\n",
    "In PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f39bfdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepVGGBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "        self.block = Conv2dNormActivation(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "            stride=stride,\n",
    "            activation_layer=None,\n",
    "            # the original model may also have groups > 1\n",
    "        )\n",
    "\n",
    "        self.shortcut = Conv2dNormActivation(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=stride,\n",
    "            activation_layer=None,\n",
    "        )\n",
    "\n",
    "        self.identity = (\n",
    "            nn.BatchNorm2d(out_channels) if in_channels == out_channels else None\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x  # <- 2x memory\n",
    "        x = self.block(x)\n",
    "        x += self.shortcut(res)\n",
    "        if self.identity:\n",
    "            x += self.identity(res)\n",
    "        x = self.relu(x)  # <- 1x memory\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfd1f8b",
   "metadata": {},
   "source": [
    "#### Reparametrization\n",
    "\n",
    "We have one `3x3` `conv->bn`, one `1x1` `conv-bn` and (somethimes) one `batchnorm`. We want to fused them together in order to create one single `conv_fused` such that `conv_fused` = `3x3conv-bn(x) + 1x1conv-bn(x) + bn(x)` or if we don't have an identity connection, `conv_fused` = `3x3conv-bn(x) + 1x1conv-bn(x)`.\n",
    "\n",
    "Let's go step by step. In order to create `conv_fused` we need to:\n",
    "- fuse the `3x3conv-bn(x)` into one `3x3conv`\n",
    "- `1x1conv-bn(x)`, then convert it to a `3x3conv`\n",
    "- convert the identity, `bn`, to a `3x3conv`\n",
    "- add all the three `3x3conv`s\n",
    "\n",
    "The first step it's easy, we can just use `get_fused_bn_to_conv_state_dict` on `RepVGGBlock.block` (the main `3x3 conv-bn`).\n",
    "\n",
    "The second step is similar, `get_fused_bn_to_conv_state_dict` on `RepVGGBlock.shortcut` (the `1x1 conv-bn`). Then we pad each kernel of the fused `1x1` by `1` in each dimension creating a `3x3`.\n",
    "\n",
    "The identity `bn` is trickier. We need to create a `3x3` `conv` that will act as an identity function and then use `get_fused_bn_to_conv_state_dict` to fuse it with the identity `bn`. This can be done by having `1` in the center of only the coresponding kernel for that coresponding channel. \n",
    "\n",
    "Recall that a conv's weight is a tensor of `in_channels, out_channels, kernel_h, kernel_w`. If we want to create an identity conv, such that `conv(x) = x`, we need to have one single `1` for that channel.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "257dec55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3, 3])\n",
      "Parameter containing:\n",
      "tensor([[[[0., 0., 0.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 0., 0.]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.randn((1,2,3,3))\n",
    "    identity_conv = nn.Conv2d(2,2,kernel_size=3, padding=1, bias=False)\n",
    "    identity_conv.weight.zero_()\n",
    "    print(identity_conv.weight.shape)\n",
    "\n",
    "    in_channels = identity_conv.in_channels\n",
    "    for i in range(in_channels):\n",
    "        identity_conv.weight[i, i % in_channels, 1, 1] = 1\n",
    "\n",
    "    print(identity_conv.weight)\n",
    "    \n",
    "    out = identity_conv(x)\n",
    "    assert torch.allclose(x, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca25e06",
   "metadata": {},
   "source": [
    "See, we created a `Conv` that acts like an identity function. \n",
    "\n",
    "Now, putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dff37045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fused_conv_state_dict_from_block(block: RepVGGBlock) -> Dict[str, Tensor]:\n",
    "    fused_block_conv_state_dict = get_fused_bn_to_conv_state_dict(\n",
    "        block.block[0], block.block[1]\n",
    "    )\n",
    "\n",
    "    if block.shortcut:\n",
    "        # fuse the 1x1 shortcut\n",
    "        conv_1x1_state_dict = get_fused_bn_to_conv_state_dict(\n",
    "            block.shortcut[0], block.shortcut[1]\n",
    "        )\n",
    "        # we pad the 1x1 to a 3x3\n",
    "        conv_1x1_state_dict[\"weight\"] = torch.nn.functional.pad(\n",
    "            conv_1x1_state_dict[\"weight\"], [1, 1, 1, 1]\n",
    "        )\n",
    "        fused_block_conv_state_dict[\"weight\"] += conv_1x1_state_dict[\"weight\"]\n",
    "        fused_block_conv_state_dict[\"bias\"] += conv_1x1_state_dict[\"bias\"]\n",
    "    if block.identity:\n",
    "        # create our identity 3x3 conv kernel\n",
    "        identify_conv = nn.Conv2d(\n",
    "            block.block[0].in_channels,\n",
    "            block.block[0].in_channels,\n",
    "            kernel_size=3,\n",
    "            bias=True,\n",
    "            padding=1,\n",
    "        ).to(block.block[0].weight.device)\n",
    "        # set them to zero!\n",
    "        identify_conv.weight.zero_()\n",
    "        # set the middle element to zero for the right channel\n",
    "        in_channels = identify_conv.in_channels\n",
    "        for i in range(identify_conv.in_channels):\n",
    "            identify_conv.weight[i, i % in_channels, 1, 1] = 1\n",
    "        # fuse the 3x3 identity\n",
    "        identity_state_dict = get_fused_bn_to_conv_state_dict(\n",
    "            identify_conv, block.identity\n",
    "        )\n",
    "        fused_block_conv_state_dict[\"weight\"] += identity_state_dict[\"weight\"]\n",
    "        fused_block_conv_state_dict[\"bias\"] += identity_state_dict[\"bias\"]\n",
    "\n",
    "    fused_conv_state_dict = {\n",
    "        k: nn.Parameter(v) for k, v in fused_block_conv_state_dict.items()\n",
    "    }\n",
    "\n",
    "    return fused_conv_state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67bdab4",
   "metadata": {},
   "source": [
    "We can now finally define a `RepVGGFastBlock`, just a `conv + relu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d80c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepVGGFastBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=stride, padding=1\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a233a62a",
   "metadata": {},
   "source": [
    "and add a `to_fast` method to `RepVGGBlock` in order to quickly create the correct `RepVGGFastBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "85163bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepVGGBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "        self.block = Conv2dNormActivation(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "            stride=stride,\n",
    "            activation_layer=None,\n",
    "            # the original model may also have groups > 1\n",
    "        )\n",
    "\n",
    "        self.shortcut = Conv2dNormActivation(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=stride,\n",
    "            activation_layer=None,\n",
    "        )\n",
    "\n",
    "        self.identity = (\n",
    "            nn.BatchNorm2d(out_channels) if in_channels == out_channels else None\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x  # <- 2x memory\n",
    "        x = self.block(x)\n",
    "        x += self.shortcut(res)\n",
    "        if self.identity:\n",
    "            x += self.identity(res)\n",
    "        x = self.relu(x)  # <- 1x memory\n",
    "        return x\n",
    "\n",
    "    def to_fast(self) -> RepVGGFastBlock:\n",
    "        fused_conv_state_dict = get_fused_conv_state_dict_from_block(self)\n",
    "        fast_block = RepVGGFastBlock(\n",
    "            self.block[0].in_channels,\n",
    "            self.block[0].out_channels,\n",
    "            stride=self.block[0].stride,\n",
    "        )\n",
    "\n",
    "        fast_block.conv.load_state_dict(fused_conv_state_dict)\n",
    "\n",
    "        return fast_block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7ca545",
   "metadata": {},
   "source": [
    "### RepVGG\n",
    "\n",
    "Let's define `RepVGGStage` and `RepVGG` with an handy `switch_to_fast` method that will swith to the fast block in-place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e0ada587",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepVGGStage(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        depth: int,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            RepVGGBlock(in_channels, out_channels, stride=2),\n",
    "            *[RepVGGBlock(out_channels, out_channels) for _ in range(depth - 1)],\n",
    "        )\n",
    "\n",
    "\n",
    "class RepVGG(nn.Sequential):\n",
    "    def __init__(self, widths: List[int], depths: List[int], in_channels: int = 3):\n",
    "        super().__init__()\n",
    "        in_out_channels = zip(widths, widths[1:])\n",
    "\n",
    "        self.stages = nn.Sequential(\n",
    "            RepVGGStage(in_channels, widths[0], depth=1),\n",
    "            *[\n",
    "                RepVGGStage(in_channels, out_channels, depth)\n",
    "                for (in_channels, out_channels), depth in zip(in_out_channels, depths)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # omit classification head for simplicity\n",
    "\n",
    "    def switch_to_fast(self):\n",
    "        for stage in self.stages:\n",
    "            for i, block in enumerate(stage):\n",
    "                stage[i] = block.to_fast()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d333dab0",
   "metadata": {},
   "source": [
    "### Let's test it out! \n",
    "\n",
    "I've created a benchmark inside `benchmark.py`, running the model on my **gtx 1080ti** with differnt batch sizes and this is the result:\n",
    "\n",
    "\n",
    "The model has two layers per stage, four stages and widths of `64, 128, 256, 512`. \n",
    "\n",
    "In their paper they scale these values by some amount (called `a` and `b`) and they used grouped convs. Since we are more interested in the reparametrization part, I skip them.\n",
    "\n",
    "![img](images/time.png)\n",
    "\n",
    "\n",
    "Yeah, so basically the reparamitrazed model takes 60/70 less time compared to the vanilla one. Wow!\n",
    "\n",
    "Let me copy and pasted the dataframe I used to store the benchmark\n",
    "\n",
    "```\n",
    "       Type    VRAM (B)  Time (s)  batch size device\n",
    "0   Default    50207232  0.140840           1   cuda\n",
    "1      Fast    48875008  0.012529           1   cuda\n",
    "2   Default    53307904  0.038717           2   cuda\n",
    "3      Fast    51165696  0.012565           2   cuda\n",
    "4   Default    61680128  0.050138           4   cuda\n",
    "5      Fast    55556608  0.012454           4   cuda\n",
    "6   Default   102623744  0.101274           8   cuda\n",
    "7      Fast    76430336  0.012429           8   cuda\n",
    "8   Default   188377600  0.171917          16   cuda\n",
    "9      Fast   135117312  0.012426          16   cuda\n",
    "10  Default   368257536  0.403996          32   cuda\n",
    "11     Fast   366115328  0.013461          32   cuda\n",
    "12  Default   676751872  0.733245          64   cuda\n",
    "13     Fast   568883712  0.013460          64   cuda\n",
    "14  Default  1330932224  1.454447         128   cuda\n",
    "15     Fast  1068743168  0.013548         128   cuda\n",
    "```\n",
    "\n",
    "You can see that the default model (multi branch) tooks `1.45`s for a `batch_size=128` while the reparamitizated one (fast) only took `0.0134`s.That is **108x** 🚀🚀🚀."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8dc5a9",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Conclusions\n",
    "In this article we have seen, step by step, how to create RepVGG; a blaziling fast model using a smart reparameterization technique.\n",
    "\n",
    "Thank you for reading it!\n",
    "\n",
    "Francesco\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
